{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-20T19:12:41.908389Z",
     "start_time": "2024-03-20T19:12:40.136917Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from models.DCNN import DCNN\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from Utils import create_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# limit GPU usage"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51e7ee162c4d26f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.cuda.set_per_process_memory_fraction(0.625)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T19:12:42.170863Z",
     "start_time": "2024-03-20T19:12:41.909394Z"
    }
   },
   "id": "6583ec646f0cdf76",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51a5db70937b662"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finsh reading data\n"
     ]
    }
   ],
   "source": [
    "root = 'Data/genres_original'\n",
    "genres = os.listdir(root)\n",
    "x = []\n",
    "y = []\n",
    "length = []\n",
    "sr = 16*1000\n",
    "for genre in genres:\n",
    "    genre_root = os.path.join(root, genre)\n",
    "    audios = os.listdir(genre_root)\n",
    "    for audio in audios:\n",
    "        audio_path = os.path.join(genre_root, audio)\n",
    "        signal, sr = librosa.load(audio_path, sr=sr)\n",
    "        x.append(signal)\n",
    "        length.append(len(signal))\n",
    "        y.append(genres.index(genre))\n",
    "min_length = min(length)\n",
    "print(\"finsh reading data\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T19:13:32.473149Z",
     "start_time": "2024-03-20T19:13:26.552704Z"
    }
   },
   "id": "688fcddfa5166508",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Segment and Normalise "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb9b1d09933e8c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 478912) (1000,)\n",
      "(8000, 1, 59049) (8000,)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    x[i] = x[i][0:min_length]\n",
    "    x[i] = librosa.util.normalize(x[i])\n",
    "x = np.asarray(x)\n",
    "y = np.asarray(y)\n",
    "# print(x.shape,y.shape)\n",
    "seg_length = 59049\n",
    "frame_num = int(x.shape[1]/seg_length)\n",
    "preprocessed_x = x[:, :frame_num*seg_length].reshape(frame_num*x.shape[0],1,seg_length)\n",
    "preprocessed_y = (y.reshape(y.shape[0],1)*np.ones((y.shape[0],frame_num))).reshape(y.shape[0]*frame_num)\n",
    "# print(preprocessed_x.shape,preprocessed_y.shape)\n",
    "print(\"finish segmentation and normalisation\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T19:13:35.706489Z",
     "start_time": "2024-03-20T19:13:32.474152Z"
    }
   },
   "id": "adde1dd7a93a75bf",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split Data "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "980160b9ea266574"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6400, 1, 59049) (6400,)\n",
      "(1600, 1, 59049) (1600,)\n",
      "(5120, 1, 59049) (5120,)\n",
      "(1280, 1, 59049) (1280,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(preprocessed_x, preprocessed_y, test_size=0.2,\n",
    "                                                    stratify=preprocessed_y,shuffle=True)\n",
    "# k-fold cross validation\n",
    "k = 5\n",
    "fold_size = x_train.shape[0]//k\n",
    "xs_train = []\n",
    "ys_train = []\n",
    "xs_valid = []\n",
    "ys_valid = []\n",
    "for i in range(k-1):\n",
    "    xs_valid.append(x_train[fold_size*i:fold_size*(i+1)])\n",
    "    ys_valid.append(y_train[fold_size*i:fold_size*(i+1)])\n",
    "    xs_train.append(np.concatenate([x_train[:fold_size*i],x_train[fold_size*(i+1):]],axis=0))\n",
    "    ys_train.append(np.concatenate([y_train[:fold_size * i],y_train[fold_size * (i + 1):]],axis=0))\n",
    "xs_valid.append(x_train[fold_size*(k-1):])\n",
    "ys_valid.append(y_train[fold_size*(k-1):])\n",
    "xs_train.append(x_train[:fold_size*(k-1)])\n",
    "ys_train.append(y_train[:fold_size*(k-1)])\n",
    "print(\"finish splitting data\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T19:13:36.977347Z",
     "start_time": "2024-03-20T19:13:35.707469Z"
    }
   },
   "id": "9beb60e3ce30783e",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Dataloaders"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e3231aec7017b9d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([6., 5., 3., ..., 2., 5., 3.])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "dataloaders_train = []\n",
    "dataloaders_valid = []\n",
    "for i in range(k):\n",
    "    dataloaders_train.append(create_dataloader(xs_train[i], ys_train[i], batch_size=batch_size))\n",
    "    dataloaders_valid.append(create_dataloader(xs_valid[i], ys_valid[i], batch_size=batch_size))\n",
    "dataloader_test = create_dataloader(x_test, y_test, batch_size=batch_size)\n",
    "print(\"finish creating dataloaders\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T19:14:07.088675Z",
     "start_time": "2024-03-20T19:14:07.069675Z"
    }
   },
   "id": "96d7659a191d6c43",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Construction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d6cbe706ea9b080"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [ConLayer: 1, Conv1d: 2, BatchNorm1d: 2, ReLU: 2]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[1;32mE:\\learning\\master\\1.2\\COMP47700SA\\Music-Genre-Classification\\venv\\lib\\site-packages\\torchinfo\\torchinfo.py:295\u001B[0m, in \u001B[0;36mforward_pass\u001B[1;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001B[0m\n\u001B[0;32m    294\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[1;32m--> 295\u001B[0m     _ \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39mx, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    296\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mdict\u001B[39m):\n",
      "File \u001B[1;32mE:\\learning\\master\\1.2\\COMP47700SA\\Music-Genre-Classification\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\learning\\master\\1.2\\COMP47700SA\\Music-Genre-Classification\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1561\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1559\u001B[0m     args \u001B[38;5;241m=\u001B[39m bw_hook\u001B[38;5;241m.\u001B[39msetup_input_hook(args)\n\u001B[1;32m-> 1561\u001B[0m result \u001B[38;5;241m=\u001B[39m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1562\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks:\n",
      "File \u001B[1;32mE:\\learning\\master\\1.2\\COMP47700SA\\Music-Genre-Classification\\DCNN.py:24\u001B[0m, in \u001B[0;36mDCNN.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m con \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcons128:\n\u001B[1;32m---> 24\u001B[0m     o \u001B[38;5;241m=\u001B[39m \u001B[43mcon\u001B[49m\u001B[43m(\u001B[49m\u001B[43mo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m con \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcons256:\n",
      "File \u001B[1;32mE:\\learning\\master\\1.2\\COMP47700SA\\Music-Genre-Classification\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\learning\\master\\1.2\\COMP47700SA\\Music-Genre-Classification\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mE:\\learning\\master\\1.2\\COMP47700SA\\Music-Genre-Classification\\DCNN.py:46\u001B[0m, in \u001B[0;36mConLayer.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 46\u001B[0m     o \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcon\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     47\u001B[0m     o \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn(o)\n",
      "File \u001B[1;32mE:\\learning\\master\\1.2\\COMP47700SA\\Music-Genre-Classification\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mE:\\learning\\master\\1.2\\COMP47700SA\\Music-Genre-Classification\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mE:\\learning\\master\\1.2\\COMP47700SA\\Music-Genre-Classification\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:310\u001B[0m, in \u001B[0;36mConv1d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    309\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\learning\\master\\1.2\\COMP47700SA\\Music-Genre-Classification\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306\u001B[0m, in \u001B[0;36mConv1d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    303\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv1d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    304\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    305\u001B[0m                     _single(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 306\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    307\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m loss_function \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[0;32m      4\u001B[0m opt \u001B[38;5;241m=\u001B[39m Adam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m)\n\u001B[1;32m----> 5\u001B[0m \u001B[43msummary\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m[\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m19683\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\learning\\master\\1.2\\COMP47700SA\\Music-Genre-Classification\\venv\\lib\\site-packages\\torchinfo\\torchinfo.py:223\u001B[0m, in \u001B[0;36msummary\u001B[1;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001B[0m\n\u001B[0;32m    216\u001B[0m validate_user_params(\n\u001B[0;32m    217\u001B[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001B[0;32m    218\u001B[0m )\n\u001B[0;32m    220\u001B[0m x, correct_input_size \u001B[38;5;241m=\u001B[39m process_input(\n\u001B[0;32m    221\u001B[0m     input_data, input_size, batch_dim, device, dtypes\n\u001B[0;32m    222\u001B[0m )\n\u001B[1;32m--> 223\u001B[0m summary_list \u001B[38;5;241m=\u001B[39m forward_pass(\n\u001B[0;32m    224\u001B[0m     model, x, batch_dim, cache_forward_pass, device, model_mode, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    225\u001B[0m )\n\u001B[0;32m    226\u001B[0m formatting \u001B[38;5;241m=\u001B[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001B[0;32m    227\u001B[0m results \u001B[38;5;241m=\u001B[39m ModelStatistics(\n\u001B[0;32m    228\u001B[0m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001B[0;32m    229\u001B[0m )\n",
      "File \u001B[1;32mE:\\learning\\master\\1.2\\COMP47700SA\\Music-Genre-Classification\\venv\\lib\\site-packages\\torchinfo\\torchinfo.py:304\u001B[0m, in \u001B[0;36mforward_pass\u001B[1;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001B[0m\n\u001B[0;32m    302\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    303\u001B[0m     executed_layers \u001B[38;5;241m=\u001B[39m [layer \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m summary_list \u001B[38;5;28;01mif\u001B[39;00m layer\u001B[38;5;241m.\u001B[39mexecuted]\n\u001B[1;32m--> 304\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    305\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to run torchinfo. See above stack traces for more details. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    306\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExecuted layers up to: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexecuted_layers\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    307\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    308\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    309\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m hooks:\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [ConLayer: 1, Conv1d: 2, BatchNorm1d: 2, ReLU: 2]"
     ]
    }
   ],
   "source": [
    "model = DCNN(10)\n",
    "model.cuda()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "opt = Adam(model.parameters(), lr=0.01)\n",
    "summary(model,[(64,1,seg_length)])\n",
    "print(\"finish model construction\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T14:44:52.976192900Z",
     "start_time": "2024-03-18T14:44:51.567647700Z"
    }
   },
   "id": "586f84b92fed94cc",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba0478e4d3367b5f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(\"-------epoch  {} -------\".format(i + 1))\n",
    "    for j in range(k):\n",
    "        print(f'fold {j+1}:')\n",
    "        loss_train = 0\n",
    "        accuracy_train = 0\n",
    "        train_size = 0\n",
    "        for batch_idx, (data, target) in enumerate(dataloaders_train[j]):\n",
    "            model.train()\n",
    "            output = model(data)\n",
    "            loss = loss_function(output, target)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            loss_train += loss.item()*len(data)\n",
    "            accuracy = (output.argmax(1) == target).sum()\n",
    "            accuracy_train += accuracy\n",
    "            train_size += len(data)\n",
    "        print(\"train set loss: {}\".format(loss_train/train_size))\n",
    "        print(\"train set accuracy: {}\".format(accuracy_train /train_size))\n",
    "\n",
    "        loss_valid = 0\n",
    "        accuracy_valid = 0\n",
    "        valid_size = 0\n",
    "        for batch_idx, (data, target) in enumerate(dataloaders_valid[j]):\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = model(data)\n",
    "                loss = loss_function(output, target)\n",
    "                loss_valid += loss.item()*len(data)\n",
    "                accuracy = (output.argmax(1) == target).sum()\n",
    "                accuracy_valid += accuracy\n",
    "                valid_size += len(data)\n",
    "        print(\"valid set Loss: {}\".format(loss_valid/valid_size))\n",
    "        print(\"valid set accuracy: {}\".format(accuracy_valid/valid_size))\n",
    "print(\"finish training\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "156ba58e4ff24277"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fd11d0b594c7140"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.eval()\n",
    "loss_test = 0\n",
    "accuracy_test = 0\n",
    "test_size = 0\n",
    "for batch_idx, (data, target) in enumerate(dataloader_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(data)\n",
    "        loss = loss_function(output, target)\n",
    "        loss_test += loss.item()*len(data)\n",
    "        accuracy = (output.argmax(1) == target).sum()\n",
    "        accuracy_test += accuracy\n",
    "        test_size += len(data)\n",
    "print(\"test set Loss: {}\".format(loss_test/test_size))\n",
    "print(\"test set accuracy: {}\".format(accuracy_test/test_size))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f65311f27441f0d6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
