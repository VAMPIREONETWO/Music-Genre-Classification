{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dn16KHyfu7EV"
   },
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "id": "MeFn93vjj9_H",
    "outputId": "f85af306-6345-4095-c1ab-62671cd539cd",
    "ExecuteTime": {
     "end_time": "2024-04-09T19:58:04.971961Z",
     "start_time": "2024-04-09T19:58:04.802906Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# please upload data directory to Google drive\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# use Google Drive to load data\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcolab\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m drive, files\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mzipfile\u001B[39;00m\n\u001B[0;32m      6\u001B[0m drive\u001B[38;5;241m.\u001B[39mmount(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/content/drive\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# please upload data directory to Google drive\n",
    "# use Google Drive to load data\n",
    "from google.colab import drive, files\n",
    "import zipfile\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "zipname = 'supports.zip'\n",
    "uploaded = files.upload()\n",
    "# Extract the zip file\n",
    "with zipfile.ZipFile(zipname, 'r') as zip_ref:\n",
    "  zip_ref.extractall()  # Extract all files to the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YcwEgH2bPTbO",
    "outputId": "34eac496-5eab-4d5f-822b-3a7c286926eb",
    "ExecuteTime": {
     "start_time": "2024-04-09T19:58:04.972962Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r \"./requirements.txt\"\n",
    "!pip install torchinfo\n",
    "!pip install torcheval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNK67Pe5Npt1"
   },
   "outputs": [],
   "source": [
    "from torch.nn import (Module, Sequential, Conv2d, BatchNorm2d, ReLU, MaxPool2d,\n",
    "                      Linear, AdaptiveAvgPool2d, BatchNorm1d, Sigmoid, Dropout)\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torcheval.metrics import MulticlassAUROC, MulticlassF1Score\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Util Methods"
   ],
   "metadata": {
    "id": "U66evfaYp-RZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNUJQROGL9O8",
    "ExecuteTime": {
     "start_time": "2024-04-09T19:58:04.973962Z"
    }
   },
   "outputs": [],
   "source": [
    "# Util\n",
    "def create_dataloader(x, y, batch_size=64, cuda=False):\n",
    "    if cuda:\n",
    "        x = torch.tensor(x, dtype=torch.float).cuda()\n",
    "        y = torch.tensor(y, dtype=torch.long).cuda()\n",
    "    else:\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "    data = TensorDataset(x, y)\n",
    "    dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def k_fold_cross_validation(x, y, k):\n",
    "    fold_size = x.shape[0] // k\n",
    "    xs_train = []\n",
    "    ys_train = []\n",
    "    xs_valid = []\n",
    "    ys_valid = []\n",
    "    for i in range(k - 1):\n",
    "        xs_valid.append(x[fold_size * i:fold_size * (i + 1)])\n",
    "        ys_valid.append(y[fold_size * i:fold_size * (i + 1)])\n",
    "        xs_train.append(np.concatenate([x[:fold_size * i], x[fold_size * (i + 1):]], axis=0))\n",
    "        ys_train.append(np.concatenate([y[:fold_size * i], y[fold_size * (i + 1):]], axis=0))\n",
    "    xs_valid.append(x[fold_size * (k - 1):])\n",
    "    ys_valid.append(y[fold_size * (k - 1):])\n",
    "    xs_train.append(x[:fold_size * (k - 1)])\n",
    "    ys_train.append(y[:fold_size * (k - 1)])\n",
    "    return xs_train, ys_train, xs_valid, ys_valid\n",
    "\n",
    "def train(model, loss_function, opt, dataloaders_train, dataloaders_valid, k, epoch=10):\n",
    "    epochs_loss = []\n",
    "    epochs_accuracy = []\n",
    "    for i in range(epoch):\n",
    "        print(\"-------epoch  {} -------\".format(i + 1))\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "\n",
    "        for j in range(k):\n",
    "            print(f'fold {j + 1}:')\n",
    "\n",
    "            # train\n",
    "            loss_train = 0\n",
    "            accuracy_train = 0\n",
    "            train_size = 0\n",
    "            for batch_idx, (data, target) in enumerate(dataloaders_train[j]):\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "                model.train()\n",
    "                output = model(data)\n",
    "                loss = loss_function(output, target)\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                data_size = len(data)\n",
    "                loss_train += loss.item() * data_size\n",
    "                accuracy_train += (output.argmax(1) == target).sum()\n",
    "                train_size += data_size\n",
    "            print(\"train set loss: {}\".format(loss_train / train_size))\n",
    "            print(\"train set accuracy: {}\".format(accuracy_train / train_size))\n",
    "\n",
    "            # valid\n",
    "            loss_valid = 0\n",
    "            accuracy_valid = 0\n",
    "            valid_size = 0\n",
    "            for batch_idx, (data, target) in enumerate(dataloaders_valid[j]):\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    output = model(data)\n",
    "                    loss = loss_function(output, target)\n",
    "                    data_size = len(data)\n",
    "                    loss_valid += loss.item() * data_size\n",
    "                    accuracy_valid += (output.argmax(1) == target).sum()\n",
    "                    valid_size += data_size\n",
    "            print(\"valid set loss: {}\".format(loss_valid / valid_size))\n",
    "            print(\"valid set accuracy: {}\".format(accuracy_valid / valid_size))\n",
    "            epoch_loss += loss_valid / valid_size\n",
    "            epoch_accuracy += accuracy_valid / valid_size\n",
    "        epoch_loss = round(epoch_loss / k, 3)\n",
    "        epoch_accuracy = round(float(epoch_accuracy) / k, 3)\n",
    "        print(f\"epoch loss: {epoch_loss}\")\n",
    "        print(f\"epoch accuracy: {epoch_accuracy}\")\n",
    "        epochs_loss.append(epoch_loss)\n",
    "        epochs_accuracy.append(epoch_accuracy)\n",
    "    return {\"loss\": epochs_loss, \"accuracy\": epochs_accuracy}\n",
    "\n",
    "\n",
    "def test(model, loss_function, dataloader_test):\n",
    "    loss_test = 0\n",
    "    accuracy_test = 0\n",
    "    AUC_test = 0\n",
    "    f1_score_test = 0\n",
    "    test_size = 0\n",
    "    for batch_idx, (data, target) in enumerate(dataloader_test):\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "            loss = loss_function(output, target)\n",
    "            loss_test += loss.item() * len(data)\n",
    "            accuracy_test += (output.argmax(1) == target).sum()\n",
    "            test_size += len(data)\n",
    "            auc = MulticlassAUROC(num_classes=10)\n",
    "            auc.update(output, target)\n",
    "            AUC_test += auc.compute() * len(data)\n",
    "            auc.reset()\n",
    "            f1 = MulticlassF1Score(num_classes=10)\n",
    "            f1.update(output, target)\n",
    "            f1_score_test += f1.compute() * len(data)\n",
    "            f1.reset()\n",
    "    loss = round(loss_test / test_size, 3)\n",
    "    accuracy = round(float(accuracy_test) / test_size,3)\n",
    "    AUC = round(float(AUC_test) / test_size, 3)\n",
    "    f1 = round(float(f1_score_test) / test_size,3)\n",
    "    print(f\"test set loss: {loss}\")\n",
    "    print(f\"test set accuracy: {accuracy}\")\n",
    "    print(f\"test set AUC: {AUC}\")\n",
    "    print(f\"test set f1-score: {f1}\")\n",
    "    return loss, accuracy, AUC, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTszVYoGQ0NO"
   },
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NcWdp47FQzeL",
    "outputId": "80b4a3c0-0b3e-4a68-9b32-26b322739711"
   },
   "outputs": [],
   "source": [
    "root = '/content/drive/MyDrive/why/genres_original' # Change according path storing data\n",
    "genres = os.listdir(root)\n",
    "x = []\n",
    "y = []\n",
    "length = []\n",
    "sr = 16*1000\n",
    "for genre in genres:\n",
    "    genre_root = os.path.join(root, genre)\n",
    "    audios = os.listdir(genre_root)\n",
    "    for audio in audios:\n",
    "        audio_path = os.path.join(genre_root, audio)\n",
    "        signal, sr = librosa.load(audio_path, sr=sr)\n",
    "        x.append(signal)\n",
    "        length.append(len(signal))\n",
    "        y.append(genres.index(genre))\n",
    "min_length = min(length)\n",
    "print(\"Finsh reading data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACZbc5UngS_r"
   },
   "source": [
    "# Segment and Normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FcHI5xnJgRbs",
    "outputId": "05ee9b03-5a1a-46ca-c381-93bcc8f07ead",
    "ExecuteTime": {
     "start_time": "2024-04-09T19:58:04.974962Z"
    }
   },
   "outputs": [],
   "source": [
    "top_db = 80\n",
    "for i in range(len(x)):\n",
    "    signal = x[i][:min_length]\n",
    "    mel_spect = librosa.feature.melspectrogram(y=signal,sr=sr,n_fft=1024) # convert signals to mel spectrogram\n",
    "    mel_spect = librosa.power_to_db(mel_spect, ref=np.max, top_db=top_db) # log compression\n",
    "    x[i] = mel_spect/-top_db # normalisation\n",
    "print(\"finish conversion and compression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-VQlxQz_nxH"
   },
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "x = np.asarray(x)\n",
    "x = x.transpose((0,2,1))\n",
    "x = x.reshape(x.shape[0],1,x.shape[1],x.shape[2])\n",
    "y = np.asarray(y)\n",
    "print(x.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kKHz9nDRjo_s",
    "outputId": "1dbffaed-d8d5-4c06-fd8b-82a56edb4927",
    "ExecuteTime": {
     "start_time": "2024-04-09T19:58:04.974962Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XynSgnsZ_nTT",
    "outputId": "16a7240e-5ea8-43c6-fdb5-bd16cbbd63c9",
    "ExecuteTime": {
     "start_time": "2024-04-09T19:58:04.975961Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,\n",
    "                                                    stratify=y,shuffle=True)\n",
    "# k-fold cross validation\n",
    "k = 5\n",
    "xs_train, ys_train, xs_valid, ys_valid = k_fold_cross_validation(x_train,y_train,k)\n",
    "print(\"finish splitting data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vj9gKZXWBFwf"
   },
   "source": [
    "# Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YTOqu20PBI6C",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9f416fad-2d7d-46cf-da86-a46539afa945",
    "ExecuteTime": {
     "start_time": "2024-04-09T19:58:04.975961Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataloaders_train = []\n",
    "dataloaders_valid = []\n",
    "for i in range(k):\n",
    "    dataloaders_train.append(create_dataloader(xs_train[i], ys_train[i], batch_size=batch_size))\n",
    "    dataloaders_valid.append(create_dataloader(xs_valid[i], ys_valid[i], batch_size=batch_size))\n",
    "dataloader_test = create_dataloader(x_test, y_test, batch_size=batch_size)\n",
    "print(\"finish creating dataloaders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QARZBdkPdvNp"
   },
   "source": [
    "# ResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41n_4pZUc3_3",
    "ExecuteTime": {
     "start_time": "2024-04-09T19:58:04.976962Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResNet(Module):\n",
    "    def __init__(self, class_num, pre_filter_size=7, in_channels=3):\n",
    "        super(ResNet18, self).__init__()\n",
    "\n",
    "        # preprocessing layer\n",
    "        self.pl = Sequential(\n",
    "            Conv2d(in_channels, 64, kernel_size=pre_filter_size, stride=2, padding=2),\n",
    "            BatchNorm2d(64),\n",
    "            ReLU(),\n",
    "            MaxPool2d(kernel_size=(3, 3), stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        # Residual Blocks\n",
    "        self.block1 = ResidualBlock(64, 64)\n",
    "        self.block2 = ResidualBlock(64, 64)\n",
    "        self.block3 = ResidualBlock(64, 128, 2)\n",
    "        self.block4 = ResidualBlock(128, 128, dropout=True)\n",
    "        self.block5 = ResidualBlock(128, 256, 2)\n",
    "        self.block6 = ResidualBlock(256, 256)\n",
    "        self.block7 = ResidualBlock(256, 512, 2)\n",
    "        self.block8 = ResidualBlock(512, 512)\n",
    "\n",
    "        self.avg_pool = AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = Sequential(Linear(512, class_num),\n",
    "                             BatchNorm1d(class_num),\n",
    "                             Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = self.pl(x)\n",
    "        o = self.block1(o)\n",
    "        o = self.block2(o)\n",
    "        o = self.block3(o)\n",
    "        o = self.block4(o)\n",
    "        o = self.block5(o)\n",
    "        o = self.block6(o)\n",
    "        o = self.block7(o)\n",
    "        o = self.block8(o)\n",
    "        o = self.avg_pool(o)\n",
    "        o = o.view(o.size(0), -1)\n",
    "        o = self.fc(o)\n",
    "        return o\n",
    "\n",
    "\n",
    "class ResidualBlock(Module):\n",
    "    def __init__(self, in_filters, out_filters, strides=1, dropout=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cl1 = Conv2d(in_filters, out_filters, kernel_size=3, stride=strides, padding=1)\n",
    "        self.bn1 = BatchNorm2d(out_filters)\n",
    "        self.relu1 = ReLU()\n",
    "\n",
    "        self.cl2 = Conv2d(out_filters, out_filters, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = BatchNorm2d(out_filters)\n",
    "\n",
    "        if strides != 1:\n",
    "            self.shortcut = Sequential(Conv2d(in_filters, out_filters, kernel_size=(1, 1), stride=strides),\n",
    "                                       BatchNorm2d(out_filters))\n",
    "        else:\n",
    "            self.shortcut = lambda x: x\n",
    "        self.relu2 = ReLU()\n",
    "        self.dropout = dropout\n",
    "        if dropout:\n",
    "            self.dp = Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = self.cl1(x)\n",
    "        o = self.bn1(o)\n",
    "        o = self.relu1(o)\n",
    "        o = self.cl2(o)\n",
    "        o = self.bn2(o)\n",
    "        o = o + self.shortcut(x)\n",
    "        o = self.relu2(o)\n",
    "        if self.dropout:\n",
    "            o = self.dp(o)\n",
    "        return o\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blfPokSwVjNf"
   },
   "source": [
    "# Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9orU3FMVh9z",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0cdb21fb-cbe1-48b4-9c02-3291edc1a137",
    "ExecuteTime": {
     "start_time": "2024-04-09T19:58:04.976962Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ResNet(10,pre_filter_size=3,in_channels=1)\n",
    "model.cuda()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "opt = Adam(model.parameters(), lr=0.001)\n",
    "summary(model,[(batch_size,1,x.shape[2],x.shape[3])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fq5fys7odqq1"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4sBdk5Gdmlx",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5ddd9852-f1e1-4f15-9cf7-d72370615663",
    "ExecuteTime": {
     "start_time": "2024-04-09T19:58:04.978012Z"
    }
   },
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "history = train(model,loss_function,opt,dataloaders_train,dataloaders_valid,k,epoch=epoch)\n",
    "print(\"finish training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzCrU5ETdmi-",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "outputId": "7ec03011-ef13-4e6b-f846-32f7444d786b",
    "ExecuteTime": {
     "start_time": "2024-04-09T19:58:04.978012Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(epoch),history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFKNSDpndmgn",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "outputId": "23715702-6c75-4c86-ec02-49cff0377143",
    "ExecuteTime": {
     "start_time": "2024-04-09T19:58:04.978962Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(epoch), history['accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test"
   ],
   "metadata": {
    "id": "_7nVGRliLob2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbSJ6zM7dmeR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "200db166-7f66-444a-c51a-e951863be07f",
    "ExecuteTime": {
     "start_time": "2024-04-09T19:58:04.978962Z"
    }
   },
   "outputs": [],
   "source": [
    "loss, acc, AUC, f1 = test(model,loss_function,dataloader_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
